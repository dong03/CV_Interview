## 卷积
稀疏交互：输出神经元只和前一层局部的神经元存在连接权重。
参数共享：卷积核和多个局部位置发生作用，使得卷积具有局部不变性。


### 卷积的输出尺寸计算公式
Output=((Input+2*pad-kernel)/stride) + 1

### 1x1 卷积的作用 与 3x3 好处
1x1实现跨通道的交互和信息整合。卷积网络通道数目的升降。
3个3 * 3卷积核叠加使用等于7 * 7的卷积核，在连通性不变的情况下，大大降低了参数个数和计算复杂度。

### 求导

### 代码实现
```
import numpy as np
import math


class Conv2D():
    def __init__(self, inputShape, outputChannel, kernelSize, stride=1, method=""):
        self.height = inputShape[1]
        self.width = inputShape[2]
        self.inputChannel = inputShape[-1]
        self.outputChannel = outputChannel
        self.batchSize = inputShape[0]
        self.stride = stride
        self.kernelSize = kernelSize
        self.method = method

        # initial the parameters of the kernel, do not initial them as zero
        self.weights = np.random.standard_normal([self.inputChannel, kernelSize, kernelSize, self.outputChannel])
        self.bias = np.random.standard_normal(self.outputChannel)

        # the shape of the output
        """
        # This part has some problems
        
        if method == "FULL":
            self.output = np.zeros(inputShape[0],
                                   math.floor((inputShape[1] - kernelSize + 2 * (kernelSize - 1)) / self.stride) + 1,
                                   math.floor((inputShape[2] - kernelSize + 2 * (kernelSize - 1)) / self.stride) + 1,
                                   self.outputChannel)  
        """

        if method == "SAME":
            self.output = np.zeros(
                (self.batchSize, math.floor(self.height / self.stride), math.floor(self.width / self.stride),
                 self.outputChannel))

        if method == "VALID":
            self.output = np.zeros([self.batchSize, math.floor((self.height - kernelSize + 1) / self.stride),
                                    math.floor((self.width - kernelSize + 1) / self.stride),
                                    self.outputChannel])

    def forward(self, x):
        weights = self.weights.reshape([-1, self.outputChannel])  # shape: [(h*w),#]

        # Filling operation
        # Note that: x is 4-dimensional.
        """
        
        if self.method == "FULL":
            x = np.pad(x, (
                (0, 0), (self.kernelSize - 1, self.kernelSize - 1), (self.kernelSize - 1, self.kernelSize - 1),
                (0, 0)), 'constant', constant_values=0)

        """
        if self.method == "SAME":
            x = np.pad(x, (
                (0, 0), (self.kernelSize // 2, self.kernelSize // 2), (self.kernelSize // 2, self.kernelSize // 2),
                (0, 0)), 'constant', constant_values=0)

        convOut = np.zeros(self.output.shape)

        for i in range(self.batchSize):
            img_i = x[i]
            # img_i = x[i][np.newaxis, :, :, :]
            colImage_i = self.im2col(img_i, self.kernelSize, self.stride)
            convOut[i] = np.reshape(np.dot(colImage_i, weights) + self.bias, self.output[0].shape)
        return convOut

    # im2col function
    def im2col(self, image, kernelSize, stride):
        imageCol = []
        for i in range(0, image.shape[0] - kernelSize + 1, stride):
            for j in range(0, image.shape[1] - kernelSize + 1, stride):
                col = image[i:i + kernelSize, j:j + kernelSize, :].reshape([-1])
                # col = image[:, i:i + kernelSize, j:j + kernelSize, :].reshape([-1])  # Do not use .view([-1])
                imageCol.append(col)
        imageCol = np.array(imageCol)  # shape: [(h*w),(c*h*w)] kernel's height, width and channels
        return imageCol


# Test part

inputData = np.random.random((4, 5, 5, 3))
print("inputShape: ", inputData.shape)
kernel = list([3, 3, 32])
print("kernel size: ", kernel)
conv2d = Conv2D(inputShape=inputData.shape, outputChannel=kernel[2], kernelSize=kernel[0], stride=1, method='VALID')
outputData = conv2d.forward(inputData)
print("outputShape: ", outputData.shape)


```

## 池化
显著降低参数量之外，能够保持平移伸缩旋转不变性。

### 求导


## 激活函数

### sigmoid,relu,tanh,softmax求导过程

https://www.cnblogs.com/steven-yang/p/6357775.html


### relu 与 sigmoid与tanh 的异同
sigmoid和 tanh均需要计算指数，复杂度高，而且会出现梯度消失。
relu可以提供网络稀疏表达能力且不会梯度消失，但是训练过程中会出现神经元死亡（负梯度置零）。
使用LeakRelu，缓解这一问题。


## 交叉熵损失的求导

https://zhuanlan.zhihu.com/p/67759205